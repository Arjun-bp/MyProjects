---
title:    "MTH783P Final Project"
author:   "Arjun Bayadegere Prabhanna, 
           (230850895)"
abstract: "This project delves into the realm of time series analysis and forecasting techniques using a dataset comprising average unit prices of avocados across diverse U.S. cities from January 2015 to March 2018. The initial stage of the project's multi-phase execution divides the dataset into subsets for testing and training. The next step is a comprehensive exploratory study that uses summary statistics, visualisation, and outlier detection to explain noteworthy data features and any abnormalities. After the dataset has undergone extensive preprocessing (Box-Cox transformation for variance stabilisation, trend elimination, seasonality, stationarity, autocorrelation tests, etc.), ACF/PACF charts help choose the optimal model. After that, the optimal model is carefully selected, validated by the AIC criterion, and verified using residual analysis to make sure the model is suitable. I ultimately selected the forecast model after calculating the forecasting error using a testing dataset. In the end, the approach provides information about avocado price prediction, which enhances avocado industry decision-making."
date:     "`r format(Sys.time(), '%d %B, %Y')`"
output:   pdf_document
fontsize: 12pt
header-includes:
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{lmodern}
  \usepackage{lipsum}
  \usepackage{fancyhdr}
  \pagestyle{fancy}
  \renewcommand{\headrulewidth}{0pt}
  \fancyhf{}
  \fancyfoot[C]{\twopagenumbers}
  \fancypagestyle{plain}{
    \renewcommand{\headrulewidth}{0pt}
    \fancyhf{}
    \fancyfoot[C]{\twopagenumbers}
  }
  \usepackage[user]{zref}
  \newcounter{pageaux}
  \def\currentauxref{PAGEAUX1}
  \newcommand{\twopagenumbers}{
    \stepcounter{pageaux}
    \thepageaux\, of\, \ref{\currentauxref}
  }
  \makeatletter
  \newcommand{\resetpageaux}{
    \clearpage
    \edef\@currentlabel{\thepageaux}\label{\currentauxref}
    \xdef\currentauxref{PAGEAUX\thepage}
    \setcounter{pageaux}{0}}
  \AtEndDvi{\edef\@currentlabel{\thepageaux}\label{\currentauxref}}
  \makeatletter
---

```{=html}
<!--
#########################################################
###         DO NOT CHANGE the following code          ###
#########################################################
-->
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{=tex}
\thispagestyle{empty}
\newpage
\setcounter{page}{1}
```
```{=html}
<!--
#########################################################
###      Start writing your report from line 61       ###
#########################################################
-->
```
# Introduction

The avocado market is a dynamic industry where prices are determined by market trends and consumer demand. This study focuses on the average avocado price across US cities from January 2015 to March 2018, using time series analysis and forecasting methodologies. Seasonality and regional variations are two of the many factors that affect avocado pricing. Our goals are to create precise forecasting models and investigate pricing trends through exploratory analysis.

# Data Preprocessing and Exploratory Data Analysis

The following steps are involved in processing the training dataset and running an exploratory analysis on it:

-   ***Data Sorting*** ***and splitting *****-** For the purpose of facilitating time series analysis, the dataset is organised logically and with care to ensure that the Date column has a consistent date format. Spliting the dataset into training and testing subsets to facilitate model evaluation.

-   ***Handling Missing Values -*** Applying the mean imputation approach to resolve any missing or NaN values found in the dataset, and then verifying for missing values to confirm the imputation process worked as intended.

-   ***Outlier Detection and Handling*** **-** Employing box plots and other pertinent plots to visually represent the outliers. Afterwards, putting procedures in place to deal with outliers, such as eliminating them or altering the data.

-   ***Summary Statistics*** **-** Computed summary statistics for numerical variables in order to comprehend their distributions, including measures of central tendency and dispersions.

-   ***Correlation Analysis*** **-** Used different plots, like scatterplots, pairwise plots, bar plots, etc., to analyse the correlation between data in order to find potential correlations or dependencies.

# Methodologies

The following steps explain the time series analysis

1.  ***Visualising the Time Series -*** In order to detect any abnormal structure, the training dataset is first transformed into a time series format and visualised to observe trend, seasonality and cyclic patterns.

    ![](images/clipboard-2310276913.png){width="298"}![](images/clipboard-2865697978.png){width="298"}

2.  ***Stationarising the Time Series -*** By calculating first order differences, the time series is stationarized by eliminating trend and seasonality. To stabilize the variance, the Box-Cox transformation is also used.

    ![](images/clipboard-3491361751.png){width="298"}![](images/clipboard-3484201156.png){width="298"}

3.  ***ACF/PACF plots to find optimal parameters -*** In the ACF plot of the differenced time series, significant autocorrelation at a specific lag indicates that the observations in the time series are correlated with their immediate previous values. In the PACF Plot, the significant partial autocorrelation at a specific lag suggests a direct effect of the previous observation on the current observation, after accounting for intervening observations.

    ![](images/clipboard-828987811.png){width="298"}![](images/clipboard-735880272.png){width="298"}

4.  ***Model fitting -*** ACF and PACF plots helped identify autocorrelation structures in the differenced series. Through careful examination, optimal parameters (P,Q) were determined to be (1,1). Evaluating multiple ARIMA models, ARIMA(2,1,1) with the lowest AIC was selected. Contrasted with auto-ARIMA, ARIMA(2,1,1) demonstrated superior fit. However, the best fitting model may not always be the best forecasting model.

5.  ***Diagnostic Tests -*** Using the Augmented Dickey-Fuller tests, we find that the auto ARIMA model's residuals have a p-value of 0.01 and the ARIMA(2,1,1) model's residuals have a p-value of 0.028. This suggests that we find the residuals to be stationary and reject the null hypothesis of non-stationarity in both scenarios.

6.  ***Model Forecasting -*** Upon applying both models to the training dataset, it is apparent that the ARIMA(2,1,1) model yielded a nearly flat forecast, while the auto arima model provided a good forecast.But, further testing was necessary before reaching any definite findings, even if the forecast was simple to depict using the Auto Arimas model.

    ![](images/clipboard-3836110131.png){width="298"}![](images/clipboard-1508690865.png){width="298"}

7.  ***Forecasting model checks -*** Upon visualising the forecasts from both models, additional testing was required to determine which model was the best. To do this, the testing dataset had to be used to calculate predicting errors like MSE and RMSE. In that case, auto arima's forecast error was lower than ARIMA's (2,1,1). In contrast to autoarima, which has MSE and RMSE of 0.04 and 0.213, ARIMA(2,1,1) has MSE and RMSE of 1.632 and 1.277.

Thus, we can conclude that, following a number of model evaluations, Auto Arima is the most appropriate and accurate model for predicting avocado prices in USD.

# Conclusion

In conclusion, the industry would beÂ benefited greatly from the helpful information we were able to obtain from our time series study of avocado pricing data. After doing thorough exploratory analysis and fitting the model, we determined that the auto-ARIMA model was the best appropriate for predicting avocado prices. Better supply chain management and pricing plans are made possible by the projected prices, which give stakeholders vital information. Even though our method worked well, there were several drawbacks, notably the reliance on the assumption of stationarity and the impact of outside variables like the state of the economy and weather. For increased accuracy, future studies may focus on adding outside variables to the predicting models.

```{=html}
<!--
#########################################################
### DO NOT CHANGE the code until the section 'R code' ###
#########################################################
-->
```
```{=tex}
\newpage
\thispagestyle{empty}
\begin{center}
\Huge \bf [END of the REPORT]
\end{center}
```
```{=tex}
\resetpageaux
\renewcommand{\thepageaux}{R-\arabic{pageaux}}
```

```{r}
# Uncomment the below code to first install the packages
# install.packages("forecast")
# install.packages("ggplot")
# install.packages("fma")
# install.packages("tseries")

# If having any issues, in knitting into 
# PDF then please install the below in the console 
#tinytex::check_installed()
```

### Caution : The images above has been saved and loaded defaultly from macbook and therefore it has a unique number. To properly knit, kinldy choose the defalut path of the image saved in your directory.

```{r}
# Loading necessary packages
library(forecast)
library(ggplot2)
library(fma)
library(tseries)
```

```{r}
# Loading the file path from directory
file_path <- "~/Downloads/avocado.csv"
avocado_df <- read.csv(file_path)
```

# Data Preprocessing and Cleaning

```{r}
# Data Sorting
# Converting Date column to Date format
avocado_df$Date <- as.Date(avocado_df$Date)
# Extracting year from Date
avocado_df$Year <- format(avocado_df$Date, "%Y")
# Sorting data before splitting
avocado_df <- avocado_df[order(avocado_df$Date), ]
```

```{r}
# Data Splitting
# Training Set
training_data <- avocado_df[avocado_df$Date <= as.Date("2017-12-31"), ]

# Testing Set
testing_data <- avocado_df[avocado_df$Date >= as.Date("2018-01-01") 
                           & avocado_df$Date <= as.Date("2018-03-11"), ]
```

```{r}
# Checking for missing or NAN values in splitted datasets
# Total number of missing values in the training dataset
train_missing_values <- sum(colSums(is.na(training_data)))
print(train_missing_values)

# Total number of missing values in testing dataset
test_missing_values <- sum(colSums(is.na(testing_data)))
print(test_missing_values)
```

```{r}
# Handling missing values by imputation method
# Selecting numeric columns for imputation in training dataset
numeric_cols_train <- sapply(training_data, is.numeric)
training_data_imputed <- training_data
training_data_imputed[, 
numeric_cols_train] <- lapply(training_data_imputed[, 
numeric_cols_train], function(x) ifelse(is.na(x), 
mean(x, na.rm = TRUE), x))

# Select numeric columns for imputation in testing dataset
numeric_cols_test <- sapply(testing_data, is.numeric)
testing_data_imputed <- testing_data
testing_data_imputed[, numeric_cols_test] <- lapply(
testing_data_imputed[, 
numeric_cols_test], function(x) ifelse(is.na(x), 
mean(x, na.rm = TRUE), x))
```

```{r}
# Checking for missing values after imputation
# Checking for missing values in imputed training dataset
training_imputed_missing <- colSums(is.na(training_data_imputed))
print(training_imputed_missing)

# Checking for missing values in imputed testing dataset
testing_imputed_missing <- colSums(is.na(testing_data_imputed))
print(testing_imputed_missing)
```

# Data Exploration using Training Dataset

```{r}
# Exploring trained dataset explicitly showing outliers
# Box plot of AveragePrice with respect to type
ggplot(training_data_imputed, aes(x = type, y = AveragePrice, fill = type))+
  geom_boxplot() +
  labs(title = "Box Plot of Average Price of Avocados by Type",
  x = "Type", y = "Average Price (USD)",
  fill = "Type") +
  theme_minimal()
```

```{r}
# Time series plot of AveragePrice with resect to Date
ggplot(training_data_imputed, aes(x = Date, y = AveragePrice)) +
geom_line() + labs(
  title = "Time Series Plot of Average Price of Avocados",
  x = "Date", 
  y = "Average Price (USD)")
```

```{r}
# Handling outliers in the data plots

# Defining IQR to show outliers
lower_percentile <- quantile(training_data_imputed$AveragePrice, 0.005)
upper_percentile <- quantile(training_data_imputed$AveragePrice, 0.995)

# Filtering outliers based on IQR
training_data_filtered <- training_data_imputed[
training_data_imputed$AveragePrice >= lower_percentile & 
training_data_imputed$
AveragePrice <= upper_percentile, ]
```

```{r}
# Box plot of AveragePrice with respect to type
ggplot(training_data_imputed, aes(x = type, y = AveragePrice, fill = type)) +
  geom_boxplot(outlier.shape = NA) +
  labs(
  title = "Box Plot of Average Price of Avocados by Type (Outliers Filtered)",
  x = "Type", y = "Average Price (USD)",
  fill = "Type") + 
  theme_minimal()
```

```{r}
# Handling outliers in the time series plot
# Calculating the mean and standard deviation of AveragePrice
mean_price <- mean(training_data_imputed$AveragePrice)
sd_price <- sd(training_data_imputed$AveragePrice)

# Defining the threshold for outliers
threshold <- 3

# Filtering out outliers
training_data_ts_filtered <- training_data_imputed[abs(
  training_data_imputed$AveragePrice - mean_price) < 
    threshold * sd_price, ]
```

```{r}
# Time Series plot of Average Price with respect to Date
ggplot(training_data_imputed, aes(x = Date, y = AveragePrice)) +
  geom_line() +
  labs(
  title = "Time Series Plot of Average Price of Avocados (Outliers Filtered)",
  x = "Date", 
  y = "Average Price (USD)")
```

```{r}
# Summary Statistics for numerical variables
summary(training_data_imputed[, numeric_cols_train])

# Summary Statistics for categorical variables
summary(training_data_imputed$type)
summary(training_data_imputed$region)
```

```{r}
# Correlation Analysis
correlation_matrix <- cor(training_data_imputed[, numeric_cols_train])
print(correlation_matrix)
```

```{r}
# Scatterplot of AveragePrice against Total Volume
ggplot(training_data_imputed, aes(x = Total.Volume, y = AveragePrice)) +
  geom_point() +
  labs(title = "Scatterplot of Average Price against Total Volume",
       x = "Total Volume", y = "Average Price") +
  theme_minimal()
```

```{r}
# Pairwise plot of selected variables
pairs(~ AveragePrice + Total.Volume + X4046 + X4225 + X4770, 
      data = training_data_imputed)
```

```{r}
# Grouped bar plot for categorical variables
ggplot(training_data_imputed, aes(
  x = type, y = AveragePrice, fill = type))+
  geom_bar(stat = "summary", fun = "mean", position = "dodge") +
  labs(title = "Grouped Bar Plot of Average Price by Avocado Type",
  x = "Avocado Type", y = "Average Price (USD)",
  fill = "Avocado Type") +
  theme_minimal()
```

```{r}
# Time Series plot of Average Price of Avocados
ggplot(training_data_imputed, aes(x = Date, y = AveragePrice)) +
  geom_line() +
  labs(title = "Time Series Plot of Average Price of Avocados",
       x = "Date", y = "Average Price (USD)") +
  theme_minimal() +
  theme(plot.title = element_text(size = 15)) +  
  theme(axis.text.x = element_text(size = 10, angle = 45, hjust = 1)) +  
  theme(axis.text.y = element_text(size = 10)) +  
  theme(axis.title = element_text(size = 12)) +  
  theme(plot.margin = margin(1, 1, 1, 1, "cm")) +  
  theme(panel.grid.minor = element_blank()) +  
  theme(panel.grid.major.x = element_blank())
```

```{r}
# Aggregating the data to weekly frequency
weekly_data <- aggregate(training_data_imputed$AveragePrice,
                         by = list(week = format(
                        training_data_imputed$Date, "%Y-%U")),
                         FUN = mean)

# Converting the aggregated data to time series objects
ts_weekly_data <- ts(weekly_data$x, start = c(2015, 1), frequency = 52)
```

# Steps for model fitting and forecasting

### 1) Time series plot observation :

```{r}
# Ploting the monthly time series data
autoplot(
  ts_weekly_data, 
  main = "Weekly Time Series Plot of Average Price of Avocados", 
  xlab = "Date", ylab = "Average Price (USD)") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
# Decomposing the Weekly time series data
avocado_decomp <- decompose(ts_weekly_data)
par(mar = c(8, 5, 4, 2) + 0.1)
plot(avocado_decomp)
```

### 2) Variance Stablisation :

```{r}
# Stablising the variance using Box-Cox Transformation
lambda <- BoxCox.lambda(ts_weekly_data)

# Plot the transformed data
plot(BoxCox(ts_weekly_data,lambda), 
    main = "Box-Cox Transformation plot")
```

### 3) Removing Trend and Seasonality :

```{r}
# Differencing the time series to remove trend and seasonality
ts_weekly_data_diff <- diff(ts_weekly_data)
par(mar = c(8, 5, 4, 2) + 0.1)
plot(decompose(ts_weekly_data_diff))
```

### 4) Model Selection :

```{r}
# Examining ACF/PACf plots
acf_result <- acf(ts_weekly_data_diff, main = "ACF Plot")
pacf_result <- pacf(ts_weekly_data_diff, main = "PACF Plot")
```

### 5) Model Fitting :

```{r}
# Model fitting using standard ARIMA
arima_model1 <- Arima(ts_weekly_data_diff, order = c(1,1,2))
print(arima_model1)

arima_model2 <- Arima(ts_weekly_data_diff, order = c(1,1,1))
print(arima_model2)

arima_model3 <- Arima(ts_weekly_data_diff, order = c(2,1,1))
print(arima_model3)

arima_model4 <- Arima(ts_weekly_data_diff, order = c(2,1,2))
print(arima_model4)
```

| Model        | AIC     | BIC     |
|--------------|---------|---------|
| ARIMA(1,1,2) | -465.43 | -453.25 |
| ARIMA(1,1,1) | -467.6  | -458.47 |
| ARIMA(2,1,1) | -468.63 | -456.46 |
| ARIMA(2,1,2) | -466.75 | -451.53 |

```{r}
# Model fitting using autoarima function
auto_arima_model <- auto.arima(
ts_weekly_data, trace = TRUE, 
approximation = TRUE, stepwise = TRUE)
print(auto_arima_model)
```

| \| Model \| AIC \| BIC \|                        |
|--------------------------------------------------|
| \| ARIMA(0,1,0)(1,1,0)[52]\| -269.31\| -264.02\| |

### 6) Diagnostic Tests :

```{r}
# Checking residuals of the best chosen models
checkresiduals(arima_model3)
checkresiduals(auto_arima_model)
```

```{r}
# Checking the residuals of the selected models using ACF/PACF
par(mfrow = c(1, 2))
acf(arima_model3$residuals)
pacf(arima_model3$residuals)

acf(auto_arima_model$residuals)
pacf(auto_arima_model$residuals)
```

```{r}
# Stationarity test for ARIMA(2,1,1)
# Performing ADF test
adf_test1 <- adf.test(arima_model3$residuals)

# Performing Ljung-Box test
ljung_box_test1 <- Box.test(
arima_model3$residuals, lag = 10, type = "Ljung-Box")

# Results
print(adf_test1)
print(ljung_box_test1)
```

```{r}
# Stationarity test for autoarima model
# Performing ADF test
adf_test_auto <- adf.test(auto_arima_model$residuals)

# Performing Ljung-Box test
ljung_box_test_auto <- Box.test(
auto_arima_model$residuals, lag = 10, type = "Ljung-Box")

# Results
print(adf_test_auto)
print(ljung_box_test_auto)
```

### 7) Model Forecasting :

```{r}
# Forecasting using ARIMA(2,1,1) model around mean 0
# Making forecasts for the first 10 weeks of 2018
forecast_result1 <- forecast(arima_model3, h = 10)
plot(forecast_result1, 
     main = "Forecast of Average Unit Price of Avocados ARIMA(2,1,1)",
     xlab = "Date", 
     ylab = "Average Price (USD)")
```

```{r}
# Forecasting using autoarima model
# Making forecasts for the first 10 weeks of 2018
forecast_result2 <- forecast(auto_arima_model, h = 10)

plot(forecast_result2, 
     main = "Forecast of Average Unit Price of Avocados Auto ARIMA", 
     xlab = "Date", 
     ylab = "Average Price (USD)")
```

### 8) Forecasting errors :

```{r}
# Subseting the testing dataset to include only the first 10 values
testing_data_subset <- head(testing_data_imputed, 10)

# Calculate Mean Squared Error (MSE)
mse_arima <- mean(
(forecast_result1$mean - testing_data_subset$AveragePrice)^2)
mse_auto_arima <- mean(
(forecast_result2$mean - testing_data_subset$AveragePrice)^2)
             
# Calculate Root Mean Squared Error (RMSE)
rmse_arima <- sqrt(mse_arima)
rmse_auto_arima <- sqrt(mse_auto_arima)

print(paste("Mean Squared Error (MSE1) - ARIMA(2,1,1):", mse_arima))
print(paste("Root Mean Squared Error (RMSE1) - ARIMA(2,1,1):", rmse_arima))
print(paste("Mean Squared Error (MSE2): - AutoArima", mse_auto_arima))
print(paste("Root Mean Squared Error (RMSE2) - AutoArima:", rmse_auto_arima))
```
